%%
%% This file is part of the `xindy'-project at the
%% Technical University Darmstadt, Computer Science Department
%% WG System Programming, Germany.
%%
%% This source is entirely written in the `noweb' literate programming
%% system.
%%
%% History at end
%%
%%
\RCS $Id$%
\RCS $Author$%
\RCS $Revision$%
\RCS $RCSfile$%
\RCS $State$%
\RCS $Date$%
%

\chapter{The \textsf{xindy}-project}

\ModuleTitle{}


\section{Introduction}

How to introduce a piece of software that is intended to process
indexes?

%\subsection{History of index processing systems}
%\subsubsection{The original {\normalfont\makeidx}-system}
%\subsubsection{How to index non-english documents?}
%\subsubsection{Who wants to index the Bible?}
%\subsection{The xindy-approach}


\section{Project realization}

In the next few sections we will give a short introduction to the
project and its realization. We will concentrate on the structure of
the project and how the system is arranged in a hierarchical
structure. We do not describe details of the implementation or give
any theoretical background here. Each module itself describes this
background, how it works and what kind of data it manipulates.

We continue with a description of the document structure and the
modularization of the \xindy-system. Thereafter follows a description
of topics that concern \Lisp as the implementation language and a
short description of the Literate Programming System (LPS) \noweb and
how to read a \noweb-source.


\subsection{Document structure and Modularization}

In this section we concentrate on the structure of the source parts of
the project. Information about other parts of the project may be found
elsewhere.

The source itself is separated into several \term{modules}. A module
is an entity of the source that contains all data descriptions and
functions that operate on those data and is intended to serve as a
separate unit with a fixed and well-documented interface and its
description. Currently the project is divided into the following
modules (higher-level modules first).
%%
\begin{deflist}{locclass}
\item[\module{xindy}] This module is the top module of the project.
\item[\module{index}] Performs building the processing of an index.
\item[\module{idxstyle}] Manages the indexstyle.
\item[\module{locref}] Deals with the low-level primitives of the
  system such as basetypes, location-references, category-attributes
  and location-classes.
\item[\module{base}] This module defines basic services.
\end{deflist}
%%
Each module depends only on modules that have a lower level in the
hierarchy.

\CL provides a mechanism to define units with a separate name space
called \term{packages}. For each of the above defined modules we
create a package with its own name space and explicitly \term{export}
objects from the package to make them accessible in other modules.

Each module is stored in a separate subdirectory of the top-level
directory. Every module may consist of several submodules which are
stored in separate files. One of those submodules is the \term{main}
submodule in which a description of the interface of the module can be
found. The convention is, that the name of the main submodule equals
the name of the module.


\subsection{The \noweb-system and automatic file generation}

Each part is written in the \noweb-LPS. The programming paradigm of
\term{Literate Programming} was first introduced by D.E.Knuth\cite{}.
Its idea is to combine program source and its documentiation in a
single file which helps to keep program and documentation consistent
and to generate a pretty-printed document of the source. We try to use
this paradigm to give all the interested people an idea of how this
system was programmed and to help ourselves in decreasing debugging
time and not ruining our nerves.

Our so far gained experience in programming with a LPS showed us that
we concentrated more on the description of our problem (\eg writing an
new function) and that the additional amount of time spend in thinking
about the problem and writing down the ideas helped us to write more
or less error-free code sections. This decreases debugging time and
overall time spend into implementation.

Every \noweb-file is written in \LaTeX{} with intermixed portions of so
called \term{code-chunks}. A code-chunk is introduced by a identifier
and a piece of code written plainly into the document. A code-chink
itself may contain the identifier of another code-chunk which is
expanded later during the tangling process. With the ability to
describe a program by a series of refinements all chunks form a tree
with one \term{root chunk}.

The process to generate a source-file from a given \noweb-file is
called \term{tangling}. A tool named \cmd{notangle} processes the
input file and expands all code chunks starting from the only root
chunk. The generated source is not intended to be read by a reader.

A program called \cmd{noweave} processes a \noweb-source (with
extension and \term{weaves} a \LaTeX-file. This file may now be
\TeX{}ed and the resulting \cmd{dvi}-file is ready for reading or
printing or whatever you like to do with.  In the top-level Makefile
we define a target that typesets the whole source as an article.



\subsection{Naming conventions}

We have to define conventions for the names of different kind of
objects to be consistent throughout the whole project.  A short
description of the conventions used in the project follows.

\subsubsection{File naming}

File naming is mainly a definition of the filename extensions. The
following extensions may accour:

\begin{deflist}{XXXX}
\item[\cmd{.nw}] the \noweb source files.
\item[\cmd{.tex}] the weaved \LaTeX{} files.
\item[\cmd{.dvi}] the \TeX{}ed documents.
\item[\cmd{.lsp}] the tangled \CL files.
\item[\cmd{.fas}] the compiled \CL sources.
\item[\cmd{.lib}] the compiled \CL libraries.
\item[\cmd{.exp}] \texttt{expect}-scripts.
\end{deflist}

We did not exceed filename length beyond 8 + 3~characters to be able
to create an easy port to MS-DOS filesystems.


\subsubsection{\Lisp naming conventions}

\subsubsection*{Identifiers}

\subsubsection*{Functions}

\subsubsection*{Slot methods}

All methods that allow access to a slot are named in a simple scheme.
[[:reader]]- and [[:accessor]]-methods are always named
[[get-]]\emph{slotname}. [[:writer]]-methods are always named
[[set-]]\emph{slotname}.

\subsubsection*{Type specifiers in the documentation}

We use the following abbreviated type-names in the \noweb-source:
%%
\begin{deflist}{XX\tstrlist}
  \item [\tint] is the type of integer numbers.
  \item [\tchar] is the type of ISO-Latin characters.
  \item [\tstring] is the type of strings composed of ISO-Latin
    characters.
  \item [\tlist] is the type of a list.
  \item [\tintlist] is the type of a list of integer numbers.
  \item [\tcharlist] is the type of a list of characters.
  \item [\tstrlist] is the type of lists containing strings.
  \item [\tclass] is the type of classes.
  \item [\tgenfunc] is the type of generic functions.
  \item [\terror] is the type of an error.
\end{deflist}
%%
Generally, if a type name has the form \textit{type}\type{-list} we
talk about a type of a list that only contains elements of type
\textit{type}.

\subsubsection{\noweb}

\subsubsection*{\noweb chunk naming}

As mentioned earlier \noweb uses \term{chunks} to name a piece of
code.  Chunks may be enlarged by additionally defining a chunk that
already appeared earlier in the source. The real source is generated
by expanding the so-called \term{root chunk}. This expansion to the
real source makes it easy so define a portion of code everywhere in
the document. We use the following naming scheme for chunks of code:

\begin{deflistit}{export interface}
\item[$*$] Root chunk of a module.
\item[Export-list of (sub-)module \emph{name}]\mbox{}\\This chunk is used to
  define the (sub-)modules interface.  interface.
\item[Submodule xxx] A submodules root-chunk.
\item[Class \emph{name}] Definition of a class.
\item[Pretty-printing] Everything that belongs to pretty-printing. It
  allows us to disable PP in a distributable environment.
\item[RCS-Identifier] Each submodule defines this identifier which is
  accessible later in the system code.
\end{deflistit}


\subsection{Regression tests with DejaGNU}

One of the major goals is to describe regression tests as part of the
source development. One can easily set up tests during development and
verify changes made to the program later by running the testsuite.
After looking around I decided to use DejaGNU because it is nearly
optimal in combination with \clisp because DejaGNU is able to connect
stdin and stdout with clisp and run the tests interactively. This
enables one to write tests that inspect internal details easily.

The regression tests are based on the \cmd{expect}-language which is a
TCL-extension. It is based on a \cmd{send}/\cmd{expect}-mechanism
which allows sending of strings to a spawned subprocess and defining
expect patterns. In our case \clisp is invoked as the subprocess and
we send \Lisp-expressions for evaluation and describe pattern that
\clisp should answer. This is a very flexible and powerful way to
achieve full control over the whole application.

In the subdirectory \file{testsuite} we find for every module a
subdirectory containing \cmd{expect}-scripts that are invoked by the
\cmd{runtool}-command. It is part of DejaGNU and invokes \cmd{expect}
on the scripts it finds in the appropriate subdirectory.

The basic configuration of \cmd{runtool} is stored in the files
\file{site.exp} and \file{config/default.exp}. After running the
regression tests the files \file{modulname.sum} and
\file{modulename.log} contain the summaries and detailed logs of the
test.


%% Local Variables:
%% mode: latex
%% TeX-master: t
%% End:
%%
%% $Log$
%% Revision 1.4  1996/01/09 14:30:32  kehr
%% Minor modifications.
%%
%% Revision 1.3  1995/12/05  18:53:16  kehr
%% Forgot to check-in some files...
%%
%% Revision 1.2  1995/11/08  16:18:46  kehr
%% Minor modifications.
%%
%% Revision 1.1  1995/11/06  16:46:09  kehr
%% Initial checkin.
%%
%%
